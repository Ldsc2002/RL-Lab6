{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Reinforcement Learning - 2024***\n",
    "*Laboratorio 6*\n",
    "Stefano Aragoni, Carol Arévalo, Luis Santos\n",
    "____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TASK 1**\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "\n",
    "**1. ¿Qué es Prioritized sweeping para ambientes determinísticos?**\n",
    "\n",
    "Prioritized sweeping es un algoritmo de planificación que se utiliza para encontrar la política óptima en un ambiente determinístico. Este algoritmo se basa en la idea de que las transiciones que tienen un mayor impacto en la política óptima son las que se deben priorizar. Para ello, se utiliza una cola de prioridad que se va actualizando a medida que se van explorando las transiciones del ambiente. De esta forma, se garantiza que las transiciones más importantes se exploren primero, lo que permite encontrar la política óptima de forma más eficiente.\n",
    "\n",
    "\n",
    "**2. ¿Qué es Trajectory Sampling?**\n",
    "\n",
    "Trajectory Sampling es un método de aprendizaje por refuerzo que se utiliza para estimar la función de valor de un estado a partir de las trayectorias de un agente en un ambiente. En este método, se generan múltiples trayectorias de un agente en el ambiente y se utilizan para estimar la función de valor de los estados visitados por el agente. De esta forma, se obtiene una estimación de la función de valor que se puede utilizar para mejorar la política del agente.\n",
    "\n",
    "\n",
    "**3. ¿Qué es Upper Confidence Bounds para Árboles (UCT por sus siglas en inglés)?**\n",
    "\n",
    "Upper Confidence Bounds para Árboles (UCT) es un algoritmo de búsqueda que se utiliza para encontrar la mejor acción a tomar en un árbol de búsqueda. Este algoritmo se basa en la idea de que se deben explorar las acciones que tienen un mayor potencial de mejorar la política del agente. Para ello, se utiliza una función de valor que combina la estimación de la recompensa de una acción con la incertidumbre asociada a esa estimación. De esta forma, se garantiza que se exploren las acciones que tienen un mayor potencial de mejorar la política del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **TASK 2**\n",
    "\n",
    "En este laboratorio, compararán el rendimiento de Dyna-Q+ y MCTS, dos de los algoritmos que vimos en clase, utilizando el entorno de FrozenLake-v1 de la biblioteca Gymnasium. Analizará y graficará las recompensas por episodio y responderá las preguntas que aparecen al final para asegurar su comprensión de los algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementación de MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendido, te proporciono una explicación revisada de cada punto sin indicar la repetición, con detalles adicionales donde sea relevante:\n",
    "\n",
    "**a. Implemente un algoritmo de búsqueda de árbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: La clase `MCTS` implementa el algoritmo MCTS completo, donde el método `Search` realiza el proceso iterativo de búsqueda. Esta búsqueda incluye selección de nodos basada en la métrica UCB1, expansión de nodos, simulación de resultados de acciones y retropropagación de las recompensas obtenidas.\n",
    "\n",
    "**b. Use una estructura de árbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementación**: Utilizando la clase `Node`, cada estado del juego se representa como un nodo en un árbol. Cada nodo puede tener varios nodos hijos, cada uno representando un estado posible después de tomar una acción específica desde el estado actual.\n",
    "\n",
    "**c. Para cada secuencia de acciones simulada, implemente una política en un estado terminal, acumule recompensas y propague estas recompensas a través del árbol.**\n",
    "- **Implementación**: Después de alcanzar un estado terminal durante la simulación (método `Simulate`), se acumulan las recompensas obtenidas. Estas recompensas se propagan hacia atrás hasta la raíz del árbol mediante el método `Backpropagate`, actualizando el conteo de visitas y la suma de recompensas de cada nodo involucrado.\n",
    "\n",
    "**d. Seleccione acciones en función de las rutas más prometedoras descubiertas durante la búsqueda.**\n",
    "- **Implementación**: La selección de nodos para expansión se basa en el valor UCB1 calculado, que favorece los nodos con alto rendimiento promedio y aquellos menos explorados, buscando el equilibrio entre explotar rutas conocidas y explorar nuevas posibilidades.\n",
    "\n",
    "**e. Considere usar límites de confianza superior para árboles (UCT) para equilibrar la exploración y la explotación en su búsqueda.**\n",
    "- **Implementación**: La fórmula UCB1 integra tanto el rendimiento promedio del nodo (explotación) como un término que crece con la raíz del logaritmo del número total de visitas al padre dividido por el número de visitas al nodo (exploración). Esta fórmula ayuda a decidir cuándo explorar nuevos caminos potencialmente prometedores frente a seguir explotando los ya conocidos.\n",
    "\n",
    "**f. Implemente un algoritmo de búsqueda de árbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: El proceso de búsqueda en `MCTS` inicia con la creación de un nodo raíz y luego itera a través de las etapas de selección, expansión, simulación y retropropagación, permitiendo que el árbol de búsqueda crezca y refine las estimaciones de las mejores acciones posibles desde el estado inicial.\n",
    "\n",
    "**g. Use una estructura de árbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementación **: La estructura del árbol permite simular trayectorias completas dentro del ambiente estocástico de FrozenLake-v1, evaluando las consecuencias de secuencias de acciones a partir de cualquier estado del juego reflejado en el árbol.\n",
    "\n",
    "**h. Para cada secuencia de acciones simulada, implemente una política en un estado terminal, acumule recompensas y propague estas recompensas a través del árbol.**\n",
    "- **Implementación**: Al final de cada simulación, la política aleatoria determina las acciones hasta alcanzar un estado terminal, momento en el cual las recompensas acumuladas proporcionan una medida de la efectividad de la secuencia de acciones tomadas. La retropropagación ajusta los parámetros estadísticos de cada nodo para reflejar su potencial basado en los resultados acumulados.\n",
    "\n",
    "**i. Seleccione acciones en función de las rutas más prometedoras descubiertas durante la búsqueda.**\n",
    "- **Implementación**: La selección y expansión continuas ajustan dinámicamente el foco de la búsqueda en el árbol, permitiendo que el algoritmo identifique y explore más a fondo las rutas que consistentemente muestran un alto rendimiento en términos de recompensas.\n",
    "\n",
    "**j. Considere usar límites de confianza superior para árboles (UCT) para equilibrar la exploración y la explotación en su búsqueda.**\n",
    "- **Implementación**: El uso de UCB1 como una guía para la selección de acciones asegura que el algoritmo de MCTS no se estanque en un localismo, pero tampoco desperdicie recursos explorando opciones claramente no rentables, logrando un compromiso entre seguridad y riesgo calculado.\n",
    "\n",
    "**k. Consideración especial: FrozenLake-v1 tiene dinámica estocástica, lo que significa que las transiciones son probabilísticas. Asegúrese de que su implementación de MCTS maneje estas transiciones probabilísticas de manera adecuada.**\n",
    "- **Consideraciones Adicionales**: El método `Simulate` utiliza acciones aleatorias, por lo que se manejar adecuadamente la incertidumbre y la variabilidad del ambiente. Este enfoque garantiza que el modelo pueda adaptarse y responder a diferentes resultados posibles a partir de un mismo estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state            # Estado actual del nodo\n",
    "        self.parent = parent          # Nodo padre\n",
    "        self.children = []            # Lista de nodos hijos\n",
    "        self.visits = 0               # Contador de visitas al nodo\n",
    "        self.wins = 0                 # Contador de victorias acumuladas\n",
    "\n",
    "    def AddChild(self, child_state):\n",
    "        child_node = Node(child_state, parent=self)  # Crea un nuevo nodo hijo\n",
    "        self.children.append(child_node)             # Añade el hijo a la lista de hijos\n",
    "        return child_node\n",
    "\n",
    "    def UCB1(self, exploration_weight=1.41):\n",
    "        if self.visits == 0:                                                                                 # Si no ha sido visitado, devuelve infinito\n",
    "            return float('inf')\n",
    "        return (self.wins / self.visits) + exploration_weight * sqrt(log(self.parent.visits) / self.visits)  # Calcula el valor UCB1 para equilibrar explotación y exploración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, max_iterations=1000):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.max_iterations = max_iterations          # Máximo de iteraciones\n",
    "\n",
    "    def Search(self, root):\n",
    "        for _ in range(self.max_iterations):          # Bucle de búsqueda\n",
    "            node = self.Select(root)                  # Selección de nodo\n",
    "            reward = self.Simulate(node.state)        # Simulación desde el estado\n",
    "            self.Backpropagate(node, reward)          # Propagación de recompensas\n",
    "\n",
    "    def Select(self, node):\n",
    "        while node.children:                                   # Mientras hay hijos\n",
    "            node = max(node.children, key=lambda n: n.UCB1())  # Selecciona el mejor hijo\n",
    "        return self.Expand(node)                               # Expande el nodo seleccionado\n",
    "\n",
    "    def Expand(self, node):\n",
    "        actions = range(self.env.action_space.n)                    # Obtiene posibles acciones\n",
    "        for action in actions:\n",
    "            next_state, reward, done, _, _ = self.env.step(action)  # Realiza la acción\n",
    "            if not done:                                            # Si no es terminal, añade el hijo\n",
    "                child_node = node.AddChild(next_state)\n",
    "        return node                                                 # Retorna el nodo expandido\n",
    "\n",
    "    def Simulate(self, state):\n",
    "        total_reward = 0\n",
    "        self.env.reset()\n",
    "        self.env.env.s = state                                      # Establece el estado actual\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()                 # Selecciona una acción al azar\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            total_reward += reward                                  # Acumula recompensas\n",
    "        return total_reward\n",
    "\n",
    "    def Backpropagate(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1                          # Incrementa las visitas\n",
    "            node.wins += reward                       # Acumula las victorias\n",
    "            node = node.parent                        # Sube al nodo padre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado final alcanzado: 4\n"
     ]
    }
   ],
   "source": [
    "def RunMCTS(envName, max_iterations):\n",
    "    env = gym.make(envName)                           # Crea el ambiente\n",
    "    initial_state = env.reset()                       # Estado inicial\n",
    "    root = Node(initial_state)                        # Nodo raíz\n",
    "    mcts = MCTS(env, max_iterations)\n",
    "    mcts.Search(root)                                 # Inicia la búsqueda\n",
    "\n",
    "    # Encuentra la mejor acción basada en las visitas\n",
    "    if root.children:\n",
    "        best_node = max(root.children, key=lambda n: n.visits)\n",
    "        return best_node.state\n",
    "    else:\n",
    "        print(\"No children found, cannot determine the best action.\")\n",
    "        return None\n",
    "\n",
    "result = RunMCTS(\"FrozenLake-v1\", 1000)\n",
    "print(\"Estado final alcanzado:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implementación de Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlus:\n",
    "    def __init__(self, env, numEpisodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1, planningSteps=10):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.numEpisodes = numEpisodes                # Número de episodios para entrenamiento\n",
    "        self.alpha = alpha                            # Tasa de aprendizaje\n",
    "        self.gamma = gamma                            # Factor de descuento\n",
    "        self.epsilon = epsilon                        # Probabilidad de exploración\n",
    "        self.planningSteps = planningSteps            # Número de pasos de planificación\n",
    "        self.qTable = np.zeros((env.observation_space.n, env.action_space.n))  # Tabla Q inicializada\n",
    "        self.model = {}                               # Modelo para almacenar transiciones\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if random.random() < self.epsilon:            # Decisión de explorar o explotar\n",
    "            return self.env.action_space.sample()     # Explorar: acción aleatoria\n",
    "        else:\n",
    "            return np.argmax(self.qTable[state])      # Explotar: mejor acción conocida\n",
    "\n",
    "    def UpdateModel(self, state, action, reward, nextState):\n",
    "        self.model[(state, action)] = (reward, nextState)             # Actualizar el modelo con la transición\n",
    "\n",
    "    def Plan(self):\n",
    "        for _ in range(self.planningSteps):                           # Realizar pasos de planificación\n",
    "            stateActionPair = random.choice(list(self.model.keys()))  # Escoger par estado-acción aleatorio\n",
    "            state, action = stateActionPair\n",
    "            reward, nextState = self.model[stateActionPair]\n",
    "            # Actualizar Q-value usando el modelo\n",
    "            self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "\n",
    "    def Train(self):\n",
    "        for episode in range(self.numEpisodes):\n",
    "            state = self.env.reset()                  # Reiniciar el ambiente\n",
    "            if isinstance(state, tuple):              # Asegurar que el estado sea un índice entero\n",
    "                state = state[0]\n",
    "            state = int(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.ChooseAction(state)                      # Elegir acción\n",
    "                nextState, reward, done, _, _ = self.env.step(action)  # Ejecutar acción\n",
    "                nextState = int(nextState)\n",
    "\n",
    "                # Actualizar Q-value usando experiencia real\n",
    "                self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "                self.UpdateModel(state, action, reward, nextState)  # Actualizar modelo\n",
    "                self.Plan()                          # Planificación usando el modelo\n",
    "                state = nextState                    # Transición al siguiente estado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def RunDynaQPlus(envName, numEpisodes):\n",
    "    env = gym.make(envName)\n",
    "    agent = DynaQPlus(env, numEpisodes=numEpisodes)\n",
    "    agent.Train()\n",
    "\n",
    "    # Evaluar la política aprendida\n",
    "    totalRewards = 0\n",
    "    for _ in range(100):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        state = int(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(agent.qTable[state])  # Acción óptima basada en Q\n",
    "            nextState, reward, done, _, _ = env.step(action)\n",
    "            nextState = int(nextState)\n",
    "            totalRewards += reward\n",
    "\n",
    "    print(\"Promedio de recompensas:\", totalRewards / 100)\n",
    "\n",
    "RunDynaQPlus('FrozenLake-v1', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Implemente el algoritmo Dyna-Q+ para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: La clase `DynaQPlus` encapsula toda la lógica de Dyna-Q+, gestionando el aprendizaje y la planificación basada en un modelo interno del entorno. Se inicializa con configuraciones específicas de hiperparámetros y se encarga de entrenar un agente para optimizar su política de decisiones a través del entrenamiento iterativo y la simulación de experiencias.\n",
    "\n",
    "**b. Use un enfoque de Q-learning para actualizaciones de valores basadas en experiencias reales.**\n",
    "- **Implementación**: Durante el proceso de entrenamiento (Train), cada paso tomado en el entorno se utiliza para actualizar la tabla Q directamente según la fórmula de Q-learning, aprovechando las recompensas y estados observados para mejorar las estimaciones de valor de acción.\n",
    "\n",
    "**c. Aprenda un modelo del entorno almacenando transiciones y recompensas para pares de estado-acción.**\n",
    "- **Implementación**: La función `UpdateModel` se utiliza para construir un diccionario que mapea pares de estado-acción a las correspondientes recompensas y estados siguientes, creando así un modelo interno del entorno que se puede utilizar para simulaciones.\n",
    "\n",
    "**d. Use el modelo aprendido para generar experiencias simuladas (pasos de planificación) y actualice los valores Q en función de estas simulaciones.**\n",
    "- **Implementación**:El método Plan realiza pasos de planificación seleccionando aleatoriamente transiciones del modelo y aplicando actualizaciones a la tabla Q basadas en estas experiencias simuladas, lo que permite un aprendizaje indirecto y mejora la política sin necesidad de interacciones adicionales en el entorno real.\n",
    "\n",
    "**e. Incorpore una bonificación de exploración en sus valores Q para fomentar la exploración de pares de estado-acción menos visitados.**\n",
    "- **Implementación**:La exploración es manejada mediante la probabilidad epsilon, que determina cuán frecuentemente se elige una acción al azar sobre la mejor acción según la tabla Q. Aunque el código proporcionado no modifica explícitamente los valores Q para reflejar una bonificación de exploración, este comportamiento puede ser implementado ajustando cómo se seleccionan las acciones durante la planificación.\n",
    "\n",
    "**f. Consideración especial: Ajuste la cantidad de pasos de planificación (parámetro 𝑛) y la bonificación de exploración para ver cómo afectan el rendimiento del aprendizaje en un entorno estocástico.**\n",
    "- **Implementación**: Los parámetros planningSteps y epsilon ofrecen un control sobre cuánto aprendizaje indirecto a través de la simulación ocurre y cuánta exploración se realiza, respectivamente. Esto permite al usuario del algoritmo afinar el balance entre exploración y explotación para optimizar el rendimiento en ambientes estocásticos como FrozenLake-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ejecución de experimentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Análisis gráfico:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Análisis:\n",
    "Compare los resultados de MCTS y Dyna-Q+.\n",
    "\n",
    "**b. Analice las fortalezas y debilidades de cada enfoque en el contexto de FrozenLake-v1.**\n",
    "\n",
    "\n",
    "**c. Considere el impacto de la naturaleza estocástica del entorno en el rendimiento de ambos algoritmos.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
