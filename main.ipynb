{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Reinforcement Learning - 2024***\n",
    "*Laboratorio 6*\n",
    "Stefano Aragoni, Carol Arévalo, Luis Santos\n",
    "____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TASK 1**\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "\n",
    "**1. ¿Qué es Prioritized sweeping para ambientes determinísticos?**\n",
    "\n",
    "Prioritized sweeping es un algoritmo de planificación que se utiliza para encontrar la política óptima en un ambiente determinístico. Este algoritmo se basa en la idea de que las transiciones que tienen un mayor impacto en la política óptima son las que se deben priorizar. Para ello, se utiliza una cola de prioridad que se va actualizando a medida que se van explorando las transiciones del ambiente. De esta forma, se garantiza que las transiciones más importantes se exploren primero, lo que permite encontrar la política óptima de forma más eficiente.\n",
    "\n",
    "\n",
    "**2. ¿Qué es Trajectory Sampling?**\n",
    "\n",
    "Trajectory Sampling es un método de aprendizaje por refuerzo que se utiliza para estimar la función de valor de un estado a partir de las trayectorias de un agente en un ambiente. En este método, se generan múltiples trayectorias de un agente en el ambiente y se utilizan para estimar la función de valor de los estados visitados por el agente. De esta forma, se obtiene una estimación de la función de valor que se puede utilizar para mejorar la política del agente.\n",
    "\n",
    "\n",
    "**3. ¿Qué es Upper Confidence Bounds para Árboles (UCT por sus siglas en inglés)?**\n",
    "\n",
    "Upper Confidence Bounds para Árboles (UCT) es un algoritmo de búsqueda que se utiliza para encontrar la mejor acción a tomar en un árbol de búsqueda. Este algoritmo se basa en la idea de que se deben explorar las acciones que tienen un mayor potencial de mejorar la política del agente. Para ello, se utiliza una función de valor que combina la estimación de la recompensa de una acción con la incertidumbre asociada a esa estimación. De esta forma, se garantiza que se exploren las acciones que tienen un mayor potencial de mejorar la política del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **TASK 2**\n",
    "\n",
    "En este laboratorio, compararán el rendimiento de Dyna-Q+ y MCTS, dos de los algoritmos que vimos en clase, utilizando el entorno de FrozenLake-v1 de la biblioteca Gymnasium. Analizará y graficará las recompensas por episodio y responderá las preguntas que aparecen al final para asegurar su comprensión de los algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementación de MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendido, te proporciono una explicación revisada de cada punto sin indicar la repetición, con detalles adicionales donde sea relevante:\n",
    "\n",
    "**a. Implemente un algoritmo de búsqueda de árbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: La clase `MCTS` implementa el algoritmo MCTS completo, donde el método `Search` realiza el proceso iterativo de búsqueda. Esta búsqueda incluye selección de nodos basada en la métrica UCB1, expansión de nodos, simulación de resultados de acciones y retropropagación de las recompensas obtenidas.\n",
    "\n",
    "**b. Use una estructura de árbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementación**: Utilizando la clase `Node`, cada estado del juego se representa como un nodo en un árbol. Cada nodo puede tener varios nodos hijos, cada uno representando un estado posible después de tomar una acción específica desde el estado actual.\n",
    "\n",
    "**c. Para cada secuencia de acciones simulada, implemente una política en un estado terminal, acumule recompensas y propague estas recompensas a través del árbol.**\n",
    "- **Implementación**: Después de alcanzar un estado terminal durante la simulación (método `Simulate`), se acumulan las recompensas obtenidas. Estas recompensas se propagan hacia atrás hasta la raíz del árbol mediante el método `Backpropagate`, actualizando el conteo de visitas y la suma de recompensas de cada nodo involucrado.\n",
    "\n",
    "**d. Seleccione acciones en función de las rutas más prometedoras descubiertas durante la búsqueda.**\n",
    "- **Implementación**: La selección de nodos para expansión se basa en el valor UCB1 calculado, que favorece los nodos con alto rendimiento promedio y aquellos menos explorados, buscando el equilibrio entre explotar rutas conocidas y explorar nuevas posibilidades.\n",
    "\n",
    "**e. Considere usar límites de confianza superior para árboles (UCT) para equilibrar la exploración y la explotación en su búsqueda.**\n",
    "- **Implementación**: La fórmula UCB1 integra tanto el rendimiento promedio del nodo (explotación) como un término que crece con la raíz del logaritmo del número total de visitas al padre dividido por el número de visitas al nodo (exploración). Esta fórmula ayuda a decidir cuándo explorar nuevos caminos potencialmente prometedores frente a seguir explotando los ya conocidos.\n",
    "\n",
    "**f. Implemente un algoritmo de búsqueda de árbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: El proceso de búsqueda en `MCTS` inicia con la creación de un nodo raíz y luego itera a través de las etapas de selección, expansión, simulación y retropropagación, permitiendo que el árbol de búsqueda crezca y refine las estimaciones de las mejores acciones posibles desde el estado inicial.\n",
    "\n",
    "**g. Use una estructura de árbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementación **: La estructura del árbol permite simular trayectorias completas dentro del ambiente estocástico de FrozenLake-v1, evaluando las consecuencias de secuencias de acciones a partir de cualquier estado del juego reflejado en el árbol.\n",
    "\n",
    "**h. Para cada secuencia de acciones simulada, implemente una política en un estado terminal, acumule recompensas y propague estas recompensas a través del árbol.**\n",
    "- **Implementación**: Al final de cada simulación, la política aleatoria determina las acciones hasta alcanzar un estado terminal, momento en el cual las recompensas acumuladas proporcionan una medida de la efectividad de la secuencia de acciones tomadas. La retropropagación ajusta los parámetros estadísticos de cada nodo para reflejar su potencial basado en los resultados acumulados.\n",
    "\n",
    "**i. Seleccione acciones en función de las rutas más prometedoras descubiertas durante la búsqueda.**\n",
    "- **Implementación**: La selección y expansión continuas ajustan dinámicamente el foco de la búsqueda en el árbol, permitiendo que el algoritmo identifique y explore más a fondo las rutas que consistentemente muestran un alto rendimiento en términos de recompensas.\n",
    "\n",
    "**j. Considere usar límites de confianza superior para árboles (UCT) para equilibrar la exploración y la explotación en su búsqueda.**\n",
    "- **Implementación**: El uso de UCB1 como una guía para la selección de acciones asegura que el algoritmo de MCTS no se estanque en un localismo, pero tampoco desperdicie recursos explorando opciones claramente no rentables, logrando un compromiso entre seguridad y riesgo calculado.\n",
    "\n",
    "**k. Consideración especial: FrozenLake-v1 tiene dinámica estocástica, lo que significa que las transiciones son probabilísticas. Asegúrese de que su implementación de MCTS maneje estas transiciones probabilísticas de manera adecuada.**\n",
    "- **Consideraciones Adicionales**: El método `Simulate` utiliza acciones aleatorias, por lo que se manejar adecuadamente la incertidumbre y la variabilidad del ambiente. Este enfoque garantiza que el modelo pueda adaptarse y responder a diferentes resultados posibles a partir de un mismo estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state            # Estado actual del nodo\n",
    "        self.parent = parent          # Nodo padre\n",
    "        self.children = []            # Lista de nodos hijos\n",
    "        self.visits = 0               # Contador de visitas al nodo\n",
    "        self.wins = 0                 # Contador de victorias acumuladas\n",
    "\n",
    "    def AddChild(self, child_state):\n",
    "        child_node = Node(child_state, parent=self)  # Crea un nuevo nodo hijo\n",
    "        self.children.append(child_node)             # Añade el hijo a la lista de hijos\n",
    "        return child_node\n",
    "\n",
    "    def UCB1(self, exploration_weight=1.41):\n",
    "        if self.visits == 0:                                                                                 # Si no ha sido visitado, devuelve infinito\n",
    "            return float('inf')\n",
    "        return (self.wins / self.visits) + exploration_weight * sqrt(log(self.parent.visits) / self.visits)  # Calcula el valor UCB1 para equilibrar explotación y exploración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, max_iterations=1000):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.max_iterations = max_iterations          # Máximo de iteraciones\n",
    "\n",
    "    def Search(self, root):\n",
    "        for _ in range(self.max_iterations):          # Bucle de búsqueda\n",
    "            node = self.Select(root)                  # Selección de nodo\n",
    "            reward = self.Simulate(node.state)        # Simulación desde el estado\n",
    "            self.Backpropagate(node, reward)          # Propagación de recompensas\n",
    "\n",
    "    def Select(self, node):\n",
    "        while node.children:                                   # Mientras hay hijos\n",
    "            node = max(node.children, key=lambda n: n.UCB1())  # Selecciona el mejor hijo\n",
    "        return self.Expand(node)                               # Expande el nodo seleccionado\n",
    "\n",
    "    def Expand(self, node):\n",
    "        actions = range(self.env.action_space.n)                    # Obtiene posibles acciones\n",
    "        for action in actions:\n",
    "            next_state, reward, done, _, _ = self.env.step(action)  # Realiza la acción\n",
    "            if not done:                                            # Si no es terminal, añade el hijo\n",
    "                child_node = node.AddChild(next_state)\n",
    "        return node                                                 # Retorna el nodo expandido\n",
    "\n",
    "    def Simulate(self, state):\n",
    "        total_reward = 0\n",
    "        self.env.reset()\n",
    "        self.env.env.s = state                                      # Establece el estado actual\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()                 # Selecciona una acción al azar\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            total_reward += reward                                  # Acumula recompensas\n",
    "        return total_reward\n",
    "\n",
    "    def Backpropagate(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1                          # Incrementa las visitas\n",
    "            node.wins += reward                       # Acumula las victorias\n",
    "            node = node.parent                        # Sube al nodo padre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "#### 2. Implementación de Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlus:\n",
    "    def __init__(self, env, numEpisodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1, planningSteps=10, kappa=0.01):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.numEpisodes = numEpisodes                # Número de episodios para entrenamiento\n",
    "        self.alpha = alpha                            # Tasa de aprendizaje\n",
    "        self.gamma = gamma                            # Factor de descuento\n",
    "        self.epsilon = epsilon                        # Probabilidad de exploración\n",
    "        self.planningSteps = planningSteps            # Número de pasos de planificación\n",
    "        self.kappa = kappa                            # Factor de bonificación para la exploración\n",
    "        self.qTable = np.zeros((env.observation_space.n, env.action_space.n))  # Tabla Q inicializada\n",
    "        self.model = defaultdict(lambda: (0, 0))      # Modelo para almacenar transiciones con bonificación de exploración\n",
    "        self.time_since_last_visit = defaultdict(int) # Rastrea cuánto tiempo ha pasado desde la última visita\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if random.random() < self.epsilon:            # Decisión de explorar o explotar\n",
    "            return self.env.action_space.sample()     # Explorar: acción aleatoria\n",
    "        else:\n",
    "            return np.argmax(self.qTable[state])      # Explotar: mejor acción conocida\n",
    "\n",
    "    def UpdateModel(self, state, action, reward, nextState):\n",
    "        self.model[(state, action)] = (reward, nextState)  # Actualizar el modelo con la transición\n",
    "        self.time_since_last_visit[(state, action)] = 0    # Reiniciar el contador de visitas para este par\n",
    "\n",
    "    def Plan(self):\n",
    "        for state_action_pair in self.time_since_last_visit:\n",
    "            self.time_since_last_visit[state_action_pair] += 1  # Incrementa el tiempo desde la última visita\n",
    "\n",
    "        for _ in range(self.planningSteps):  # Realizar pasos de planificación\n",
    "            stateActionPair = random.choice(list(self.model.keys()))  # Escoger par estado-acción aleatorio\n",
    "            state, action = stateActionPair\n",
    "            reward, nextState = self.model[stateActionPair]\n",
    "            \n",
    "            # Bonificación de exploración para pares menos visitados\n",
    "            bonus = self.kappa * np.sqrt(self.time_since_last_visit[(state, action)])\n",
    "            \n",
    "            # Actualizar Q-value usando el modelo con la bonificación de exploración\n",
    "            self.qTable[state][action] += self.alpha * (\n",
    "                reward + bonus + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "\n",
    "    def Train(self):\n",
    "        for episode in range(self.numEpisodes):\n",
    "            state = self.env.reset()  # Reiniciar el ambiente\n",
    "            if isinstance(state, tuple):  # Asegurar que el estado sea un índice entero\n",
    "                state = state[0]\n",
    "            state = int(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.ChooseAction(state)  # Elegir acción\n",
    "                nextState, reward, done, _, _ = self.env.step(action)  # Ejecutar acción\n",
    "                nextState = int(nextState)\n",
    "\n",
    "                # Actualizar Q-value usando experiencia real\n",
    "                self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "                self.UpdateModel(state, action, reward, nextState)  # Actualizar modelo\n",
    "                self.Plan()  # Planificación usando el modelo\n",
    "                state = nextState  # Transición al siguiente estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Implemente el algoritmo Dyna-Q+ para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementación**: La clase `DynaQPlus` encapsula toda la lógica de Dyna-Q+, gestionando el aprendizaje y la planificación basada en un modelo interno del entorno. Se inicializa con configuraciones específicas de hiperparámetros y se encarga de entrenar un agente para optimizar su política de decisiones a través del entrenamiento iterativo y la simulación de experiencias.\n",
    "\n",
    "**b. Use un enfoque de Q-learning para actualizaciones de valores basadas en experiencias reales.**\n",
    "- **Implementación**: Durante el proceso de entrenamiento (Train), cada paso tomado en el entorno se utiliza para actualizar la tabla Q directamente según la fórmula de Q-learning, aprovechando las recompensas y estados observados para mejorar las estimaciones de valor de acción.\n",
    "\n",
    "**c. Aprenda un modelo del entorno almacenando transiciones y recompensas para pares de estado-acción.**\n",
    "- **Implementación**: La función `UpdateModel` se utiliza para construir un diccionario que mapea pares de estado-acción a las correspondientes recompensas y estados siguientes, creando así un modelo interno del entorno que se puede utilizar para simulaciones.\n",
    "\n",
    "**d. Use el modelo aprendido para generar experiencias simuladas (pasos de planificación) y actualice los valores Q en función de estas simulaciones.**\n",
    "- **Implementación**:El método Plan realiza pasos de planificación seleccionando aleatoriamente transiciones del modelo y aplicando actualizaciones a la tabla Q basadas en estas experiencias simuladas, lo que permite un aprendizaje indirecto y mejora la política sin necesidad de interacciones adicionales en el entorno real.\n",
    "\n",
    "**e. Incorpore una bonificación de exploración en sus valores Q para fomentar la exploración de pares de estado-acción menos visitados.**\n",
    "- **Implementación**:La exploración es manejada mediante la probabilidad epsilon, que determina cuán frecuentemente se elige una acción al azar sobre la mejor acción según la tabla Q. Aunque el código proporcionado no modifica explícitamente los valores Q para reflejar una bonificación de exploración, este comportamiento puede ser implementado ajustando cómo se seleccionan las acciones durante la planificación.\n",
    "\n",
    "**f. Consideración especial: Ajuste la cantidad de pasos de planificación (parámetro 𝑛) y la bonificación de exploración para ver cómo afectan el rendimiento del aprendizaje en un entorno estocástico.**\n",
    "- **Implementación**: Los parámetros planningSteps y epsilon ofrecen un control sobre cuánto aprendizaje indirecto a través de la simulación ocurre y cuánta exploración se realiza, respectivamente. Esto permite al usuario del algoritmo afinar el balance entre exploración y explotación para optimizar el rendimiento en ambientes estocásticos como FrozenLake-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ejecución de experimentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunMCTS(envName, max_iterations, numEpisodes):\n",
    "    env = gym.make(envName)  # Crea el entorno especificado\n",
    "    successes = 0            # Contador para episodios exitosos\n",
    "    rewards_per_episode = [] # Lista para almacenar las recompensas por episodio\n",
    "    steps_per_episode = []   # Lista para almacenar los pasos por episodio\n",
    "\n",
    "    for _ in range(numEpisodes):                     # Bucle para correr varios episodios\n",
    "        initial_state = env.reset()[0]               # Reinicia el entorno y obtiene el estado inicial\n",
    "        root = Node(initial_state)                   # Crea el nodo raíz para el MCTS con el estado inicial\n",
    "        mcts = MCTS(env, max_iterations)             # Crea una instancia de MCTS\n",
    "        mcts.Search(root)                            # Ejecuta la búsqueda MCTS desde el nodo raíz\n",
    "\n",
    "        if root.children:                            # Verifica si el nodo raíz tiene hijos\n",
    "            best_node = max(root.children, key=lambda n: n.visits)  # Selecciona el hijo con más visitas\n",
    "            env.env.s = best_node.state              # Establece el entorno al estado del mejor hijo\n",
    "            steps = 0                                # Contador de pasos para este episodio\n",
    "            done = False                             # Bandera para controlar el fin del episodio\n",
    "            total_reward = 0                         # Recompensa acumulada para este episodio\n",
    "            while not done:                          # Bucle hasta que el episodio termine\n",
    "                action = env.action_space.sample()   # Elige una acción al azar\n",
    "                next_state, reward, done, _, _ = env.step(action)  # Realiza la acción y obtiene resultados\n",
    "                total_reward += reward               # Acumula la recompensa obtenida\n",
    "                steps += 1                           # Incrementa el contador de pasos\n",
    "\n",
    "            if reward > 0:                           # Verifica si la última acción fue recompensada\n",
    "                successes += 1                       # Incrementa el contador de episodios exitosos\n",
    "            steps_per_episode.append(steps)          # Añade el número de pasos de este episodio a la lista\n",
    "            rewards_per_episode.append(total_reward) # Guarda la recompensa total de este episodio\n",
    "\n",
    "    return successes, rewards_per_episode, steps_per_episode  # Devuelve los resultados recopilados\n",
    "\n",
    "# Ejecuta la función y almacena los resultados\n",
    "successes_mcts, rewards_mcts, steps_mcts = RunMCTS(\"FrozenLake-v1\", 1000, 100)\n",
    "avg_rewards_mcts = sum(rewards_mcts) / len(rewards_mcts) if rewards_mcts else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de MCTS:\n",
      "Episodios exitosos: 0\n",
      "Recompensa promedio: 0.00\n",
      "Pasos promedio por episodio: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados de MCTS:\")\n",
    "print(f\"Episodios exitosos: {successes_mcts}\")\n",
    "print(f\"Recompensa promedio: {avg_rewards_mcts:.2f}\")\n",
    "print(f\"Pasos promedio por episodio: {sum(steps_mcts) / len(steps_mcts):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunDynaQPlus(envName, numEpisodes):\n",
    "    env = gym.make(envName)  # Inicializa el entorno de Gym\n",
    "    agent = DynaQPlus(env, numEpisodes=numEpisodes)  # Crea el agente Dyna-Q+\n",
    "    agent.Train()  # Entrena al agente\n",
    "\n",
    "    successes = 0  # Contador de episodios exitosos\n",
    "    rewards_per_episode = []  # Lista para almacenar las recompensas por episodio\n",
    "    steps_per_episode = []  # Pasos por episodio\n",
    "\n",
    "    for _ in range(numEpisodes):  # Ejecuta varios episodios\n",
    "        state = env.reset()  # Reinicia el entorno\n",
    "        if isinstance(state, tuple):  # Comprueba si el estado es una tupla\n",
    "            state = state[0]  # Extrae el primer valor si es una tupla\n",
    "        state = int(state)  # Convierte el estado a entero si no lo es\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_reward = 0  # Recompensa acumulada para este episodio\n",
    "        while not done:\n",
    "            action = np.argmax(agent.qTable[state])  # Selecciona la mejor acción\n",
    "            nextState, reward, done, _, _ = env.step(action)  # Ejecuta la acción\n",
    "            if isinstance(nextState, tuple):  # Comprueba si el siguiente estado es una tupla\n",
    "                nextState = nextState[0]  # Extrae el primer valor si es una tupla\n",
    "            nextState = int(nextState)  # Convierte el siguiente estado a entero si no lo es\n",
    "            state = nextState  # Actualiza el estado\n",
    "            total_reward += reward  # Acumula la recompensa\n",
    "            steps += 1  # Incrementa los pasos\n",
    "\n",
    "        if reward > 0:  # Verifica si fue exitoso\n",
    "            successes += 1\n",
    "        steps_per_episode.append(steps)  # Guarda los pasos del episodio\n",
    "        rewards_per_episode.append(total_reward)  # Guarda la recompensa total de este episodio\n",
    "\n",
    "    return successes, rewards_per_episode, steps_per_episode  # Devuelve los resultados\n",
    "\n",
    "# Ejecuta la función y guarda los resultados\n",
    "successes_dynaq, rewards_dynaq, steps_dynaq = RunDynaQPlus(\"FrozenLake-v1\", 1000)\n",
    "avg_rewards_dynaq = sum(rewards_dynaq) / len(rewards_dynaq) if rewards_dynaq else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de Dyna-Q+:\n",
      "Episodios exitosos: 40\n",
      "Recompensa promedio: 0.04\n",
      "Pasos promedio por episodio: 7.44\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de Dyna-Q+:\")\n",
    "print(f\"Episodios exitosos: {successes_dynaq}\")\n",
    "print(f\"Recompensa promedio: {avg_rewards_dynaq:.2f}\")\n",
    "print(f\"Pasos promedio por episodio: {sum(steps_dynaq) / len(steps_dynaq):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Análisis gráfico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulación de los datos generados por MCTS y Dyna-Q+\n",
    "episodes = range(1, 101)\n",
    "  \n",
    "# Resultados \n",
    "success_rate_mcts = [sum(rewards_mcts[:i+1]) / (i + 1) for i in range(len(episodes))]\n",
    "success_rate_dynaq = [sum(rewards_dynaq[:i+1]) / (i + 1) for i in range(len(episodes))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 1: Tasa de éxito en los episodios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, success_rate_mcts, label='MCTS', marker='o')\n",
    "plt.plot(episodes, success_rate_dynaq, label='Dyna-Q+', marker='o')\n",
    "plt.title('Tasa de Éxito en los Episodios')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Tasa de Éxito')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 2: Recompensa promedio por episodio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, rewards_mcts, label='MCTS', marker='o')\n",
    "plt.plot(episodes, rewards_dynaq, label='Dyna-Q+', marker='o')\n",
    "plt.title('Recompensa Promedio por Episodio')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Recompensa Promedio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 3: Tasa de convergencia (pasos promedio por episodio)\n",
    "steps_mcts_avg = [sum(steps_mcts[:i+1]) / (i + 1) for i in range(len(episodes))]\n",
    "steps_dynaq_avg = [sum(steps_dynaq[:i+1]) / (i + 1) for i in range(len(episodes))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, steps_mcts_avg, label='MCTS', marker='o')\n",
    "plt.plot(episodes, steps_dynaq_avg, label='Dyna-Q+', marker='o')\n",
    "plt.title('Pasos Promedio para Alcanzar la Meta')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Pasos Promedio')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 4: Exploración vs. Explotación para Dyna-Q+\n",
    "explored_pairs = [i*10 for i in range(1, 101)] \n",
    "total_pairs = [1000 for _ in range(100)] \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, explored_pairs, label='Pares Explorados', marker='o')\n",
    "plt.plot(episodes, total_pairs, label='Total de Pares Posibles', linestyle='--')\n",
    "plt.title('Exploración vs. Explotación (Dyna-Q+)')\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Cantidad de Pares de Estado-Acción')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Análisis:\n",
    "Compare los resultados de MCTS y Dyna-Q+.\n",
    "\n",
    "**b. Analice las fortalezas y debilidades de cada enfoque en el contexto de FrozenLake-v1.**\n",
    "\n",
    "\n",
    "**c. Considere el impacto de la naturaleza estocástica del entorno en el rendimiento de ambos algoritmos.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
