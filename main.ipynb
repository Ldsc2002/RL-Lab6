{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6\n",
    "\n",
    "## Autores\n",
    "- Luis Santos\n",
    "- José Miguel González\n",
    "- Carol Arévalo\n",
    "- Stefano Aragoni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "**1. ¿Qué es Prioritized sweeping para ambientes determinísticos?**\n",
    "\n",
    "Prioritized sweeping es un algoritmo de planificación que se utiliza para encontrar la política óptima en un ambiente determinístico. Este algoritmo se basa en la idea de que las transiciones que tienen un mayor impacto en la política óptima son las que se deben priorizar. Para ello, se utiliza una cola de prioridad que se va actualizando a medida que se van explorando las transiciones del ambiente. De esta forma, se garantiza que las transiciones más importantes se exploren primero, lo que permite encontrar la política óptima de forma más eficiente.\n",
    "\n",
    "\n",
    "**2. ¿Qué es Trajectory Sampling?**\n",
    "\n",
    "Trajectory Sampling es un método de aprendizaje por refuerzo que se utiliza para estimar la función de valor de un estado a partir de las trayectorias de un agente en un ambiente. En este método, se generan múltiples trayectorias de un agente en el ambiente y se utilizan para estimar la función de valor de los estados visitados por el agente. De esta forma, se obtiene una estimación de la función de valor que se puede utilizar para mejorar la política del agente.\n",
    "\n",
    "\n",
    "**3. ¿Qué es Upper Confidence Bounds para Árboles (UCT por sus siglas en inglés)?**\n",
    "\n",
    "Upper Confidence Bounds para Árboles (UCT) es un algoritmo de búsqueda que se utiliza para encontrar la mejor acción a tomar en un árbol de búsqueda. Este algoritmo se basa en la idea de que se deben explorar las acciones que tienen un mayor potencial de mejorar la política del agente. Para ello, se utiliza una función de valor que combina la estimación de la recompensa de una acción con la incertidumbre asociada a esa estimación. De esta forma, se garantiza que se exploren las acciones que tienen un mayor potencial de mejorar la política del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "En este laboratorio, compararán el rendimiento de Dyna-Q+ y MCTS, dos de los algoritmos que vimos en clase, utilizando el entorno de FrozenLake-v1 de la biblioteca Gymnasium. Analizará y graficará las recompensas por episodio y responderá las preguntas que aparecen al final para asegurar su comprensión de los algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado final alcanzado: 0\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "\n",
    "    def AddChild(self, child_state):\n",
    "        child_node = Node(child_state, parent=self)\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def UCB1(self, exploration_weight=1.41):\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        return (self.wins / self.visits) + exploration_weight * sqrt(log(self.parent.visits) / self.visits)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, env, max_iterations=1000):\n",
    "        self.env = env\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def Search(self, root):\n",
    "        for _ in range(self.max_iterations):\n",
    "            node = self.Select(root)\n",
    "            reward = self.Simulate(node.state)\n",
    "            self.Backpropagate(node, reward)\n",
    "\n",
    "    def Select(self, node):\n",
    "        while node.children:\n",
    "            node = max(node.children, key=lambda n: n.UCB1())\n",
    "        return self.Expand(node)\n",
    "\n",
    "    def Expand(self, node):\n",
    "        actions = range(self.env.action_space.n)\n",
    "        for action in actions:\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            # Only add child if the state is not terminal\n",
    "            if not done:\n",
    "                child_node = node.AddChild(next_state)\n",
    "        return node  # Return the expanded node\n",
    "\n",
    "    def Simulate(self, state):\n",
    "        total_reward = 0\n",
    "        self.env.reset()\n",
    "        self.env.env.s = state  # Set the current state\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()  # Random action\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "        return total_reward\n",
    "\n",
    "    def Backpropagate(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.wins += reward\n",
    "            node = node.parent\n",
    "\n",
    "def RunMCTS(envName, max_iterations):\n",
    "    env = gym.make(envName)\n",
    "    initial_state = env.reset()\n",
    "    root = Node(initial_state)\n",
    "    mcts = MCTS(env, max_iterations)\n",
    "    mcts.Search(root)\n",
    "\n",
    "    # Find the best action based on the visits\n",
    "    if root.children:\n",
    "        best_node = max(root.children, key=lambda n: n.visits)\n",
    "        return best_node.state\n",
    "    else:\n",
    "        print(\"No children found, cannot determine the best action.\")\n",
    "        return None\n",
    "\n",
    "result = RunMCTS(\"FrozenLake-v1\", 1000)\n",
    "print(\"Estado final alcanzado:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: 0.0\n"
     ]
    }
   ],
   "source": [
    "class DynaQPlus:\n",
    "    def __init__(self, env, numEpisodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1, planningSteps=10):\n",
    "        self.env = env\n",
    "        self.numEpisodes = numEpisodes\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration probability\n",
    "        self.planningSteps = planningSteps  # Number of planning steps\n",
    "        self.qTable = np.zeros((env.observation_space.n, env.action_space.n))  # Q-table\n",
    "        self.model = {}  # Model for storing state transitions\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.qTable[state])  # Exploit\n",
    "\n",
    "    def UpdateModel(self, state, action, reward, nextState):\n",
    "        self.model[(state, action)] = (reward, nextState)\n",
    "\n",
    "    def Plan(self):\n",
    "        for _ in range(self.planningSteps):\n",
    "            # Sample a random state-action pair from the model\n",
    "            stateActionPair = random.choice(list(self.model.keys()))\n",
    "            state, action = stateActionPair\n",
    "            reward, nextState = self.model[stateActionPair]\n",
    "            # Update Q-value using the model\n",
    "            self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "\n",
    "    def Train(self):\n",
    "        for episode in range(self.numEpisodes):\n",
    "            state = self.env.reset()  # Reset environment\n",
    "            if isinstance(state, tuple):  # Check if state is a tuple\n",
    "                state = state[0]  # Get the first element (the state)\n",
    "            state = int(state)  # Ensure state is an integer index\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.ChooseAction(state)  # Choose action\n",
    "                nextState, reward, done, _, _ = self.env.step(action)  # Take action\n",
    "                nextState = int(nextState)  # Ensure nextState is an integer index\n",
    "\n",
    "                # Update Q-value using real experience\n",
    "                self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "                # Update model\n",
    "                self.UpdateModel(state, action, reward, nextState)\n",
    "                # Plan using the model\n",
    "                self.Plan()\n",
    "                state = nextState  # Transition to the next state\n",
    "\n",
    "def RunDynaQPlus(envName, numEpisodes):\n",
    "    env = gym.make(envName)\n",
    "    agent = DynaQPlus(env, numEpisodes=numEpisodes)\n",
    "    agent.Train()\n",
    "\n",
    "    # Evaluate the learned policy\n",
    "    totalRewards = 0\n",
    "    for _ in range(100):  # Run 100 episodes to evaluate\n",
    "        state = env.reset()  # Reset environment\n",
    "        if isinstance(state, tuple):  # Check if state is a tuple\n",
    "            state = state[0]  # Get the first element (the state)\n",
    "        state = int(state)  # Ensure state is an integer index\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(agent.qTable[state])  # Choose the best action\n",
    "            nextState, reward, done, _, _ = env.step(action)\n",
    "            nextState = int(nextState)  # Ensure nextState is an integer index\n",
    "            totalRewards += reward\n",
    "            state = nextState\n",
    "    \n",
    "    print(\"Promedio de recompensas:\", totalRewards / 100)\n",
    "\n",
    "RunDynaQPlus('FrozenLake-v1', 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
