{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Reinforcement Learning - 2024***\n",
    "*Laboratorio 6*\n",
    "Stefano Aragoni, Carol Ar茅valo, Luis Santos\n",
    "____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TASK 1**\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo m谩s completamente posible.\n",
    "\n",
    "\n",
    "**1. 驴Qu茅 es Prioritized sweeping para ambientes determin铆sticos?**\n",
    "\n",
    "Prioritized sweeping es un algoritmo de planificaci贸n que se utiliza para encontrar la pol铆tica 贸ptima en un ambiente determin铆stico. Este algoritmo se basa en la idea de que las transiciones que tienen un mayor impacto en la pol铆tica 贸ptima son las que se deben priorizar. Para ello, se utiliza una cola de prioridad que se va actualizando a medida que se van explorando las transiciones del ambiente. De esta forma, se garantiza que las transiciones m谩s importantes se exploren primero, lo que permite encontrar la pol铆tica 贸ptima de forma m谩s eficiente.\n",
    "\n",
    "\n",
    "**2. 驴Qu茅 es Trajectory Sampling?**\n",
    "\n",
    "Trajectory Sampling es un m茅todo de aprendizaje por refuerzo que se utiliza para estimar la funci贸n de valor de un estado a partir de las trayectorias de un agente en un ambiente. En este m茅todo, se generan m煤ltiples trayectorias de un agente en el ambiente y se utilizan para estimar la funci贸n de valor de los estados visitados por el agente. De esta forma, se obtiene una estimaci贸n de la funci贸n de valor que se puede utilizar para mejorar la pol铆tica del agente.\n",
    "\n",
    "\n",
    "**3. 驴Qu茅 es Upper Confidence Bounds para rboles (UCT por sus siglas en ingl茅s)?**\n",
    "\n",
    "Upper Confidence Bounds para rboles (UCT) es un algoritmo de b煤squeda que se utiliza para encontrar la mejor acci贸n a tomar en un 谩rbol de b煤squeda. Este algoritmo se basa en la idea de que se deben explorar las acciones que tienen un mayor potencial de mejorar la pol铆tica del agente. Para ello, se utiliza una funci贸n de valor que combina la estimaci贸n de la recompensa de una acci贸n con la incertidumbre asociada a esa estimaci贸n. De esta forma, se garantiza que se exploren las acciones que tienen un mayor potencial de mejorar la pol铆tica del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **TASK 2**\n",
    "\n",
    "En este laboratorio, comparar谩n el rendimiento de Dyna-Q+ y MCTS, dos de los algoritmos que vimos en clase, utilizando el entorno de FrozenLake-v1 de la biblioteca Gymnasium. Analizar谩 y graficar谩 las recompensas por episodio y responder谩 las preguntas que aparecen al final para asegurar su comprensi贸n de los algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementaci贸n de MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendido, te proporciono una explicaci贸n revisada de cada punto sin indicar la repetici贸n, con detalles adicionales donde sea relevante:\n",
    "\n",
    "**a. Implemente un algoritmo de b煤squeda de 谩rbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementaci贸n**: La clase `MCTS` implementa el algoritmo MCTS completo, donde el m茅todo `Search` realiza el proceso iterativo de b煤squeda. Esta b煤squeda incluye selecci贸n de nodos basada en la m茅trica UCB1, expansi贸n de nodos, simulaci贸n de resultados de acciones y retropropagaci贸n de las recompensas obtenidas.\n",
    "\n",
    "**b. Use una estructura de 谩rbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementaci贸n**: Utilizando la clase `Node`, cada estado del juego se representa como un nodo en un 谩rbol. Cada nodo puede tener varios nodos hijos, cada uno representando un estado posible despu茅s de tomar una acci贸n espec铆fica desde el estado actual.\n",
    "\n",
    "**c. Para cada secuencia de acciones simulada, implemente una pol铆tica en un estado terminal, acumule recompensas y propague estas recompensas a trav茅s del 谩rbol.**\n",
    "- **Implementaci贸n**: Despu茅s de alcanzar un estado terminal durante la simulaci贸n (m茅todo `Simulate`), se acumulan las recompensas obtenidas. Estas recompensas se propagan hacia atr谩s hasta la ra铆z del 谩rbol mediante el m茅todo `Backpropagate`, actualizando el conteo de visitas y la suma de recompensas de cada nodo involucrado.\n",
    "\n",
    "**d. Seleccione acciones en funci贸n de las rutas m谩s prometedoras descubiertas durante la b煤squeda.**\n",
    "- **Implementaci贸n**: La selecci贸n de nodos para expansi贸n se basa en el valor UCB1 calculado, que favorece los nodos con alto rendimiento promedio y aquellos menos explorados, buscando el equilibrio entre explotar rutas conocidas y explorar nuevas posibilidades.\n",
    "\n",
    "**e. Considere usar l铆mites de confianza superior para 谩rboles (UCT) para equilibrar la exploraci贸n y la explotaci贸n en su b煤squeda.**\n",
    "- **Implementaci贸n**: La f贸rmula UCB1 integra tanto el rendimiento promedio del nodo (explotaci贸n) como un t茅rmino que crece con la ra铆z del logaritmo del n煤mero total de visitas al padre dividido por el n煤mero de visitas al nodo (exploraci贸n). Esta f贸rmula ayuda a decidir cu谩ndo explorar nuevos caminos potencialmente prometedores frente a seguir explotando los ya conocidos.\n",
    "\n",
    "**f. Implemente un algoritmo de b煤squeda de 谩rbol de Monte Carlo (MCTS) para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementaci贸n**: El proceso de b煤squeda en `MCTS` inicia con la creaci贸n de un nodo ra铆z y luego itera a trav茅s de las etapas de selecci贸n, expansi贸n, simulaci贸n y retropropagaci贸n, permitiendo que el 谩rbol de b煤squeda crezca y refine las estimaciones de las mejores acciones posibles desde el estado inicial.\n",
    "\n",
    "**g. Use una estructura de 谩rbol para simular diferentes secuencias de acciones a partir del estado actual.**\n",
    "- **Implementaci贸n **: La estructura del 谩rbol permite simular trayectorias completas dentro del ambiente estoc谩stico de FrozenLake-v1, evaluando las consecuencias de secuencias de acciones a partir de cualquier estado del juego reflejado en el 谩rbol.\n",
    "\n",
    "**h. Para cada secuencia de acciones simulada, implemente una pol铆tica en un estado terminal, acumule recompensas y propague estas recompensas a trav茅s del 谩rbol.**\n",
    "- **Implementaci贸n**: Al final de cada simulaci贸n, la pol铆tica aleatoria determina las acciones hasta alcanzar un estado terminal, momento en el cual las recompensas acumuladas proporcionan una medida de la efectividad de la secuencia de acciones tomadas. La retropropagaci贸n ajusta los par谩metros estad铆sticos de cada nodo para reflejar su potencial basado en los resultados acumulados.\n",
    "\n",
    "**i. Seleccione acciones en funci贸n de las rutas m谩s prometedoras descubiertas durante la b煤squeda.**\n",
    "- **Implementaci贸n**: La selecci贸n y expansi贸n continuas ajustan din谩micamente el foco de la b煤squeda en el 谩rbol, permitiendo que el algoritmo identifique y explore m谩s a fondo las rutas que consistentemente muestran un alto rendimiento en t茅rminos de recompensas.\n",
    "\n",
    "**j. Considere usar l铆mites de confianza superior para 谩rboles (UCT) para equilibrar la exploraci贸n y la explotaci贸n en su b煤squeda.**\n",
    "- **Implementaci贸n**: El uso de UCB1 como una gu铆a para la selecci贸n de acciones asegura que el algoritmo de MCTS no se estanque en un localismo, pero tampoco desperdicie recursos explorando opciones claramente no rentables, logrando un compromiso entre seguridad y riesgo calculado.\n",
    "\n",
    "**k. Consideraci贸n especial: FrozenLake-v1 tiene din谩mica estoc谩stica, lo que significa que las transiciones son probabil铆sticas. Aseg煤rese de que su implementaci贸n de MCTS maneje estas transiciones probabil铆sticas de manera adecuada.**\n",
    "- **Consideraciones Adicionales**: El m茅todo `Simulate` utiliza acciones aleatorias, por lo que se manejar adecuadamente la incertidumbre y la variabilidad del ambiente. Este enfoque garantiza que el modelo pueda adaptarse y responder a diferentes resultados posibles a partir de un mismo estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state            # Estado actual del nodo\n",
    "        self.parent = parent          # Nodo padre\n",
    "        self.children = []            # Lista de nodos hijos\n",
    "        self.visits = 0               # Contador de visitas al nodo\n",
    "        self.wins = 0                 # Contador de victorias acumuladas\n",
    "\n",
    "    def AddChild(self, child_state):\n",
    "        child_node = Node(child_state, parent=self)  # Crea un nuevo nodo hijo\n",
    "        self.children.append(child_node)             # A帽ade el hijo a la lista de hijos\n",
    "        return child_node\n",
    "\n",
    "    def UCB1(self, exploration_weight=1.41):\n",
    "        if self.visits == 0:                                                                                 # Si no ha sido visitado, devuelve infinito\n",
    "            return float('inf')\n",
    "        return (self.wins / self.visits) + exploration_weight * sqrt(log(self.parent.visits) / self.visits)  # Calcula el valor UCB1 para equilibrar explotaci贸n y exploraci贸n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, env, max_iterations=1000):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.max_iterations = max_iterations          # M谩ximo de iteraciones\n",
    "\n",
    "    def Search(self, root):\n",
    "        for _ in range(self.max_iterations):          # Bucle de b煤squeda\n",
    "            node = self.Select(root)                  # Selecci贸n de nodo\n",
    "            reward = self.Simulate(node.state)        # Simulaci贸n desde el estado\n",
    "            self.Backpropagate(node, reward)          # Propagaci贸n de recompensas\n",
    "\n",
    "    def Select(self, node):\n",
    "        while node.children:                                   # Mientras hay hijos\n",
    "            node = max(node.children, key=lambda n: n.UCB1())  # Selecciona el mejor hijo\n",
    "        return self.Expand(node)                               # Expande el nodo seleccionado\n",
    "\n",
    "    def Expand(self, node):\n",
    "        actions = range(self.env.action_space.n)                    # Obtiene posibles acciones\n",
    "        for action in actions:\n",
    "            next_state, reward, done, _, _ = self.env.step(action)  # Realiza la acci贸n\n",
    "            if not done:                                            # Si no es terminal, a帽ade el hijo\n",
    "                child_node = node.AddChild(next_state)\n",
    "        return node                                                 # Retorna el nodo expandido\n",
    "\n",
    "    def Simulate(self, state):\n",
    "        total_reward = 0\n",
    "        self.env.reset()\n",
    "        self.env.env.s = state                                      # Establece el estado actual\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.env.action_space.sample()                 # Selecciona una acci贸n al azar\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            total_reward += reward                                  # Acumula recompensas\n",
    "        return total_reward\n",
    "\n",
    "    def Backpropagate(self, node, reward):\n",
    "        while node is not None:\n",
    "            node.visits += 1                          # Incrementa las visitas\n",
    "            node.wins += reward                       # Acumula las victorias\n",
    "            node = node.parent                        # Sube al nodo padre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado final alcanzado: 4\n"
     ]
    }
   ],
   "source": [
    "def RunMCTS(envName, max_iterations):\n",
    "    env = gym.make(envName)                           # Crea el ambiente\n",
    "    initial_state = env.reset()                       # Estado inicial\n",
    "    root = Node(initial_state)                        # Nodo ra铆z\n",
    "    mcts = MCTS(env, max_iterations)\n",
    "    mcts.Search(root)                                 # Inicia la b煤squeda\n",
    "\n",
    "    # Encuentra la mejor acci贸n basada en las visitas\n",
    "    if root.children:\n",
    "        best_node = max(root.children, key=lambda n: n.visits)\n",
    "        return best_node.state\n",
    "    else:\n",
    "        print(\"No children found, cannot determine the best action.\")\n",
    "        return None\n",
    "\n",
    "result = RunMCTS(\"FrozenLake-v1\", 1000)\n",
    "print(\"Estado final alcanzado:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implementaci贸n de Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQPlus:\n",
    "    def __init__(self, env, numEpisodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1, planningSteps=10):\n",
    "        self.env = env                                # Ambiente de Gym\n",
    "        self.numEpisodes = numEpisodes                # N煤mero de episodios para entrenamiento\n",
    "        self.alpha = alpha                            # Tasa de aprendizaje\n",
    "        self.gamma = gamma                            # Factor de descuento\n",
    "        self.epsilon = epsilon                        # Probabilidad de exploraci贸n\n",
    "        self.planningSteps = planningSteps            # N煤mero de pasos de planificaci贸n\n",
    "        self.qTable = np.zeros((env.observation_space.n, env.action_space.n))  # Tabla Q inicializada\n",
    "        self.model = {}                               # Modelo para almacenar transiciones\n",
    "\n",
    "    def ChooseAction(self, state):\n",
    "        if random.random() < self.epsilon:            # Decisi贸n de explorar o explotar\n",
    "            return self.env.action_space.sample()     # Explorar: acci贸n aleatoria\n",
    "        else:\n",
    "            return np.argmax(self.qTable[state])      # Explotar: mejor acci贸n conocida\n",
    "\n",
    "    def UpdateModel(self, state, action, reward, nextState):\n",
    "        self.model[(state, action)] = (reward, nextState)             # Actualizar el modelo con la transici贸n\n",
    "\n",
    "    def Plan(self):\n",
    "        for _ in range(self.planningSteps):                           # Realizar pasos de planificaci贸n\n",
    "            stateActionPair = random.choice(list(self.model.keys()))  # Escoger par estado-acci贸n aleatorio\n",
    "            state, action = stateActionPair\n",
    "            reward, nextState = self.model[stateActionPair]\n",
    "            # Actualizar Q-value usando el modelo\n",
    "            self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "\n",
    "    def Train(self):\n",
    "        for episode in range(self.numEpisodes):\n",
    "            state = self.env.reset()                  # Reiniciar el ambiente\n",
    "            if isinstance(state, tuple):              # Asegurar que el estado sea un 铆ndice entero\n",
    "                state = state[0]\n",
    "            state = int(state)\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.ChooseAction(state)                      # Elegir acci贸n\n",
    "                nextState, reward, done, _, _ = self.env.step(action)  # Ejecutar acci贸n\n",
    "                nextState = int(nextState)\n",
    "\n",
    "                # Actualizar Q-value usando experiencia real\n",
    "                self.qTable[state][action] += self.alpha * (reward + self.gamma * np.max(self.qTable[nextState]) - self.qTable[state][action])\n",
    "                self.UpdateModel(state, action, reward, nextState)  # Actualizar modelo\n",
    "                self.Plan()                          # Planificaci贸n usando el modelo\n",
    "                state = nextState                    # Transici贸n al siguiente estado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def RunDynaQPlus(envName, numEpisodes):\n",
    "    env = gym.make(envName)\n",
    "    agent = DynaQPlus(env, numEpisodes=numEpisodes)\n",
    "    agent.Train()\n",
    "\n",
    "    # Evaluar la pol铆tica aprendida\n",
    "    totalRewards = 0\n",
    "    for _ in range(100):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        state = int(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(agent.qTable[state])  # Acci贸n 贸ptima basada en Q\n",
    "            nextState, reward, done, _, _ = env.step(action)\n",
    "            nextState = int(nextState)\n",
    "            totalRewards += reward\n",
    "\n",
    "    print(\"Promedio de recompensas:\", totalRewards / 100)\n",
    "\n",
    "RunDynaQPlus('FrozenLake-v1', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Implemente el algoritmo Dyna-Q+ para resolver el entorno FrozenLake-v1.**\n",
    "- **Implementaci贸n**: La clase `DynaQPlus` encapsula toda la l贸gica de Dyna-Q+, gestionando el aprendizaje y la planificaci贸n basada en un modelo interno del entorno. Se inicializa con configuraciones espec铆ficas de hiperpar谩metros y se encarga de entrenar un agente para optimizar su pol铆tica de decisiones a trav茅s del entrenamiento iterativo y la simulaci贸n de experiencias.\n",
    "\n",
    "**b. Use un enfoque de Q-learning para actualizaciones de valores basadas en experiencias reales.**\n",
    "- **Implementaci贸n**: Durante el proceso de entrenamiento (Train), cada paso tomado en el entorno se utiliza para actualizar la tabla Q directamente seg煤n la f贸rmula de Q-learning, aprovechando las recompensas y estados observados para mejorar las estimaciones de valor de acci贸n.\n",
    "\n",
    "**c. Aprenda un modelo del entorno almacenando transiciones y recompensas para pares de estado-acci贸n.**\n",
    "- **Implementaci贸n**: La funci贸n `UpdateModel` se utiliza para construir un diccionario que mapea pares de estado-acci贸n a las correspondientes recompensas y estados siguientes, creando as铆 un modelo interno del entorno que se puede utilizar para simulaciones.\n",
    "\n",
    "**d. Use el modelo aprendido para generar experiencias simuladas (pasos de planificaci贸n) y actualice los valores Q en funci贸n de estas simulaciones.**\n",
    "- **Implementaci贸n**:El m茅todo Plan realiza pasos de planificaci贸n seleccionando aleatoriamente transiciones del modelo y aplicando actualizaciones a la tabla Q basadas en estas experiencias simuladas, lo que permite un aprendizaje indirecto y mejora la pol铆tica sin necesidad de interacciones adicionales en el entorno real.\n",
    "\n",
    "**e. Incorpore una bonificaci贸n de exploraci贸n en sus valores Q para fomentar la exploraci贸n de pares de estado-acci贸n menos visitados.**\n",
    "- **Implementaci贸n**:La exploraci贸n es manejada mediante la probabilidad epsilon, que determina cu谩n frecuentemente se elige una acci贸n al azar sobre la mejor acci贸n seg煤n la tabla Q. Aunque el c贸digo proporcionado no modifica expl铆citamente los valores Q para reflejar una bonificaci贸n de exploraci贸n, este comportamiento puede ser implementado ajustando c贸mo se seleccionan las acciones durante la planificaci贸n.\n",
    "\n",
    "**f. Consideraci贸n especial: Ajuste la cantidad de pasos de planificaci贸n (par谩metro ) y la bonificaci贸n de exploraci贸n para ver c贸mo afectan el rendimiento del aprendizaje en un entorno estoc谩stico.**\n",
    "- **Implementaci贸n**: Los par谩metros planningSteps y epsilon ofrecen un control sobre cu谩nto aprendizaje indirecto a trav茅s de la simulaci贸n ocurre y cu谩nta exploraci贸n se realiza, respectivamente. Esto permite al usuario del algoritmo afinar el balance entre exploraci贸n y explotaci贸n para optimizar el rendimiento en ambientes estoc谩sticos como FrozenLake-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ejecuci贸n de experimentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. An谩lisis gr谩fico:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. An谩lisis:\n",
    "Compare los resultados de MCTS y Dyna-Q+.\n",
    "\n",
    "**b. Analice las fortalezas y debilidades de cada enfoque en el contexto de FrozenLake-v1.**\n",
    "\n",
    "\n",
    "**c. Considere el impacto de la naturaleza estoc谩stica del entorno en el rendimiento de ambos algoritmos.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
